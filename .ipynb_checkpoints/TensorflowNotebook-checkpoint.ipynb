{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"data/covtype.csv\")\n",
    "data = df[[\n",
    "    'Elevation',\n",
    "    'Aspect',\n",
    "    'Slope',\n",
    "    'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology',\n",
    "    'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am',\n",
    "    'Hillshade_Noon',\n",
    "    'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points',\n",
    "    'Cover_Type'\n",
    "]].copy()\n",
    "\n",
    "num_pip = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.1)\n",
    "train_data_x = train_data.drop(\"Cover_Type\", axis=1)\n",
    "train_data_x = num_pip.fit_transform(train_data_x)\n",
    "train_data_y = train_data[\"Cover_Type\"].copy() - 1\n",
    "\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5)\n",
    "val_data_x = val_data.drop(\"Cover_Type\", axis=1)\n",
    "val_data_x = num_pip.transform(val_data_x)\n",
    "val_data_y = val_data[\"Cover_Type\"].copy() - 1\n",
    "test_data_x = test_data.drop(\"Cover_Type\", axis=1)\n",
    "test_data_x = num_pip.transform(test_data_x)\n",
    "test_data_y = test_data[\"Cover_Type\"].copy() - 1\n",
    "n_outputs = len(train_data_y.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow model\n",
    "BATCH_SIZE = 128\n",
    "X = tf.placeholder(tf.float32, shape=(None, 10), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(BATCH_SIZE).repeat()\n",
    "itera = dataset.make_initializable_iterator()\n",
    "features, labels = itera.get_next()\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(\n",
    "        inputs=features,\n",
    "        units=256,\n",
    "        activation=tf.nn.elu\n",
    "    )\n",
    "    hidden2 = tf.layers.dense(\n",
    "        inputs=hidden1,\n",
    "        units=64,\n",
    "        activation=tf.nn.elu\n",
    "    )\n",
    "    hidden3 = tf.layers.dense(\n",
    "        inputs=hidden2,\n",
    "        units=16,\n",
    "        activation=tf.nn.elu\n",
    "    )\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=hidden3,\n",
    "        units=n_outputs,\n",
    "        activation=None\n",
    "    )\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels,\n",
    "        logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"model/\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "n_batches = train_data_x.shape[0] // BATCH_SIZE\n",
    "#n_bacths_validation = val_data_x.shape[0] // BATCH_SIZE\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    sess.run(itera.initializer, feed_dict={X: train_data_x, y: train_data_y.values})\n",
    "    for i in range(EPOCHS):\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "        for batch_index in range(n_batches):\n",
    "            _, loss_val, acc_val = sess.run([training_op, loss, accuracy])\n",
    "            acc_total += acc_val\n",
    "            loss_total += loss_val\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str_loss = loss_summary.eval()\n",
    "                summary_str_xentropy = xentropy_summary.eval()\n",
    "                summary_str_accuracy = accuracy_summary.eval()\n",
    "                step = i * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str_loss, step)\n",
    "                file_writer.add_summary(summary_str_accuracy, step)\n",
    "        if (i+1) % 5 == 0:\n",
    "            \n",
    "            print(\"Train - Epoch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1, loss_total/n_batches, acc_total/n_batches))\n",
    "    \n",
    "#     sess.run(iter.initializer, feed_dict={ x: val_data_x.values, y: val_data_y.values})\n",
    "#     acc_total_val = 0\n",
    "#     loss_total_val = 0\n",
    "#     for _ in range(n_bacths_validation):\n",
    "#         loss_valida, acc_valida = sess.run([loss, accuracy])\n",
    "#         acc_total_val += acc_valida\n",
    "#         loss_total_val += loss_valida\n",
    "#     print(\"Val - Epoch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1, loss_total_val, acc_total_val))\n",
    "    \n",
    "    save_path = saver.save(sess, \"model/final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
